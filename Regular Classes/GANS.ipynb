{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a0b71d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets.mnist import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8e44ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_discriminator(input_shape = (28, 28, 1)):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(20, (5, 5), padding = 'same', input_shape = input_shape))\n",
    "    model.add(keras.layers.LeakyReLU(alpha = 0.1))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Conv2D(20, (5, 5), padding = 'same', input_shape = input_shape))    # Change input size to input shape \n",
    "    model.add(keras.layers.LeakyReLU(alpha = 0.1))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Conv2D(20, (5, 5), padding = 'same', input_shape = input_shape))\n",
    "    model.add(keras.layers.LeakyReLU(alpha = 0.1))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Conv2D(20, (5, 5), padding = 'same', input_shape = input_shape))\n",
    "    model.add(keras.layers.LeakyReLU(alpha = 0.1))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5b3ee6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_generator(latent_dim):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(150, input_shape = (latent_dim,)))\n",
    "    model.add(keras.layers.LeakyReLU(alpha = 0.1))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(300, input_shape = (latent_dim,)))\n",
    "    model.add(keras.layers.LeakyReLU(alpha = 0.1))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(450, input_shape = (latent_dim,)))\n",
    "    model.add(keras.layers.LeakyReLU(alpha = 0.1))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(600, input_shape = (latent_dim,)))    #convert latent_dim to an array (latent_dim,)\n",
    "    model.add(keras.layers.LeakyReLU(alpha = 0.1))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(784, input_shape = (latent_dim,)))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Reshape((28, 28, 1)))\n",
    "    # model.add(keras.activations.sigmoid())\n",
    "    model.add(keras.layers.Activation('sigmoid'))  #change this here , use .layers as well, above gives errors \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb970fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gans(g_model, d_model):\n",
    "    d_model.trainable = False\n",
    "    model = keras.Sequential()\n",
    "    model.add(g_model)\n",
    "    model.add(d_model)\n",
    "    \n",
    "    model.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8194be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_real_samples():\n",
    "    (train_x,_),(_,_)=load_data()\n",
    "    train_x=np.expand_dims(train_x,axis=-1)\n",
    "    train_x=train_x.astype('float32')\n",
    "    train_x=train_x/255\n",
    "\n",
    "    return train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeff7a3c-9c27-46f0-99b1-44f2014ed4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_samples(dataset,n_samples):\n",
    "    ids=np.random.randint(0,dataset.shape[0],n_samples)\n",
    "    dataset=dataset[ids]\n",
    "    y=np.ones((n_samples,1))\n",
    "    return dataset,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "927964d0-5322-4e34-b993-b754a31646f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_noise(latent_dim,n_samples):\n",
    "    noise=np.random.randn(latent_dim*n_samples)\n",
    "    noise=noise.reshape(n_samples,latent_dim)\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af9dd6e9-02f6-4dda-8c3d-67863fb388c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_samples(g_model,latent_dim,n_samples):\n",
    "    noise=generate_latent_noise(latent_dim,n_samples)\n",
    "    fake_data=g_model.predict(noise)\n",
    "    y=np.zeros((n_samples,1))\n",
    "    return fake_data, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1a6de31-a6f5-4c34-9518-92f9ea7d509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(g_model,d_model,dataset,latent_dim,epoch,n_samples=30):\n",
    "    x_real,y_real=generate_real_samples(dataset,n_samples)\n",
    "    loss_real,acc_real=d_model.evaluate(x_real,y_real)       #spelling error for evaluate\n",
    "    x_fake,y_fake=generate_fake_samples(g_model,latent_dim,n_samples)\n",
    "    loss_fake,acc_fake=d_model.evaluate(x_fake,y_fake)\n",
    "    print(f'Accuracy real:{acc_real*100}%, Accuracy fake:{acc_fake*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c294ea2f-6ba1-463e-819a-f2f8a69d2674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g_model,d_model,define_gan,dataset,latent_dim,n_epoch=10,n_batch=500):\n",
    "    batches_per_epoch=int(dataset.shape[0]/n_batch)\n",
    "    n_samples=250\n",
    "    for i in range(n_epoch):\n",
    "        for j in range(batches_per_epoch):\n",
    "            x_real,y_real=generate_real_samples(dataset,n_samples)\n",
    "            x_fake,y_fake=generate_fake_samples(g_model,latent_dim,n_samples)\n",
    "\n",
    "            d_loss_real,_=d_model.train_on_batch(x_real,y_real)\n",
    "            d_loss_fake,_=d_model.train_on_batch(x_fake,y_fake)\n",
    "\n",
    "            d_loss=d_loss_fake+d_loss_real\n",
    "\n",
    "            noise=generate_latent_noise(latent_dim,n_samples)\n",
    "\n",
    "            y_gan=np.ones((n_samples,1))   # replace -1 with 1\n",
    "            g_loss,_=define_gan.train_on_batch(noise,y_gan)\n",
    "            print(f'on epoch:{i+1}, Disc Loss: {d_loss}, Gen Loss: {g_loss}')\n",
    "\n",
    "            if i % 10 ==0:\n",
    "                summary(g_model,d_model,dataset,latent_dim,i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e690cdd3-7006-4859-a891-b323b066949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim=100\n",
    "d_model=define_discriminator()\n",
    "g_model=define_generator(latent_dim)\n",
    "gan=define_gans(g_model, d_model)\n",
    "dataset=load_real_samples()\n",
    "train(g_model,d_model,gan,dataset,latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077db9b2-ec87-4d76-8c93-f0982ee3032a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
